{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95cfbd26-7fa9-4dd7-9367-c27e3c9e03bb",
   "metadata": {},
   "source": [
    "# Leveraging Lakehouse data with Amazon SageMaker XGBoost and AutoML\n",
    "_**Supervised learning with MLFlow logging of experiments**_\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Prepration](#Preparation)\n",
    "1. [Data Preparation](#DataPreparation)\n",
    "1. [Training XGBoost](#XGBoost)\n",
    "1. [Training AutoML](#AutoML)\n",
    "1. [Deployment and inference test](#Deployment_and_inference_test)\n",
    "1. [Evaluation](#Evaluation)\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "One of the key advantages of the new SageMaker AI Unified Studio is its ability to integrate data from multiple sources. In this notebook, we'll walk through an example of bringing data from a Lakehouse to train models using XGBoost and AutoML. We'll also leverage the power of MLFlow servers to capture and analyze the training data.\n",
    "\n",
    "This notebook demonstrates how to predict a customer's purchase potential based on a set of features. We'll go through the following steps:\n",
    "\n",
    "* Setting up your Amazon SageMaker AI notebook\n",
    "* Querying data sources using Athena\n",
    "* Transforming the data to feed into Amazon SageMaker algorithms\n",
    "* Training a model using the Gradient Boosting algorithm (XGBoost)\n",
    "* Launching an AutoML task to target the same feature\n",
    "* Utilizing MLFlow to capture and visualize experiment data\n",
    "\n",
    "---\n",
    "\n",
    "## Preparation\n",
    "\n",
    "Let's start by bringing in the Python libraries that we'll use throughout the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d777a36-e56e-467a-a69e-817c57fee926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import sagemaker\n",
    "import mlflow\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "from sagemaker.modules import Session\n",
    "from sagemaker_studio import Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa126ff-222b-4796-b14f-e8044a6e361d",
   "metadata": {},
   "source": [
    "Now, let's set up our logging and specify the necessary configurations:\n",
    "\n",
    "1. Configure the logging we'll use, including the ARN of the MLFlow server we've set up in the prerequisite\n",
    "2. Specify the S3 bucket and prefix for storing training and model data\n",
    "3. Set up the IAM role ARN to provide necessary permissions for training and hosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daba65bb-43b8-4e54-92e9-9c7e33073559",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd639c27-1a78-4507-b98f-cdb418ab87fe",
   "metadata": {},
   "source": [
    "### Copy MLFlow Tracking Server ARN\n",
    "\n",
    "Copy/Paste the ARN of your Project MLFlow Tracking Server. You can find it by navigating to the Project->Compute page, then selecting \"MLFlow Tracking Server\" tab. Select the `Copy ARN` button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4d6000-5e19-4c0c-a6ab-7a5455317b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = Project()\n",
    "#mlflow_arn = project.mlflow_tracking_server_arn\n",
    "\n",
    "# Cut/Paste the ARN from the Tracking Server instance\n",
    "## mlflow_arn = \"arn:aws:sagemaker:us-west-2:767398116961:mlflow-tracking-server/tracking-server-blogxwjruvstqo-cv0wvz63pbj11s-dev\"\n",
    "mlflow_arn = \"COPY_TRACKING_SERVER_ARN_HERE\"\n",
    "print(f\"ARN: {mlflow_arn}\")\n",
    "\n",
    "mlflow.set_tracking_uri(mlflow_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96facd5",
   "metadata": {},
   "source": [
    "One of the added benefit of SageMaker Unified Studio is the use of Project to bring resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff00e488-7591-4446-9444-788d9df49f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize AWS session\n",
    "session = boto3.Session()\n",
    "bucket_root = project.s3.root\n",
    "role = project.iam_role\n",
    "\n",
    "# Parse the S3 URI\n",
    "s3_parts = bucket_root.replace(\"s3://\", \"\").split(\"/\")\n",
    "bucket = s3_parts[0]\n",
    "prefix = \"/\".join(s3_parts[1:])\n",
    "\n",
    "## If you prefer NOT using the new SageMaker AI Project framework, here is an alternative\n",
    "#session = sagemaker.Session()\n",
    "#bucket = session.default_bucket()\n",
    "#from sagemaker import get_execution_role\n",
    "#role = get_execution_role()\n",
    "#sagemaker_client = boto3.Session().client(service_name='sagemaker',region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7310961-92a5-4bef-a3c8-57a5cd11a1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Using Bucket: {bucket}\")\n",
    "print(f\"Using prefix: {prefix}\")\n",
    "print(f\"Using Role: {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a51b428",
   "metadata": {},
   "source": [
    "Now, let's retrieve the name of the project's database through the default catalog:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea29364f-1c0d-4ddd-8f24-5068c06f615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A good example of the Project class is getting the name of the project's database through the default catalog\n",
    "catalog = project.connection().catalog()\n",
    "project_database = catalog.databases[0].name\n",
    "project_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf429cae-55ac-4db9-819e-272462ef217d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: If your account has more than one Catalog, use this code to lookup names\n",
    "id = 0\n",
    "for db in catalog.databases:\n",
    "    print(f\"Index {id}: {db}\")\n",
    "    id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eccfa13-9ec6-445a-85a6-f153bcb3ab0d",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "First, we need to upload the data file named \"5000-sales-records.csv\". If you are running this notebook from the **Sagemaker Unified Studio Workshop**, this file can be downloaded from the instructions page. Next, we can upload the file using the S3 Browser on the Project->Data page. Once the file is successfully uploaded, open the S3 Console, and locate the file by navigating to the folder prefix where you uploaded it. (Note: file uploads can normally be found under the `local-uploads` prefix).\n",
    "\n",
    "From the S3 console, select the file \"5000-sales-records.csv\" and hit \"Copy S3 URI\" button. Then paste the URI within the quotes in the read_csv() call below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc19b755-4880-4dba-8f23-fd7b490db2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pandas to read CSV directly from S3 URI\n",
    "\n",
    "# Example:\n",
    "# data = pd.read_csv(\"s3://csv-file-store-72f9fec0/dzd_d22v67c8i2tzv4/blogxwjruvstqo/dev/local-uploads/1756921917470/5000-sales-records.csv\")\n",
    "data = pd.read_csv(\"COPY_S3_URI_HERE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d866bb71-4f54-49fc-ad67-023f3668f32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to match Spark Dataframe infer\n",
    "data.rename(columns={\n",
    "    \"Region\": \"region\",\n",
    "    \"Country\": \"country\",\n",
    "    \"Item Type\": \"item type\",\n",
    "    \"Sales Channel\": \"sales channel\",\n",
    "    \"Order Priority\": \"order priority\",\n",
    "    \"Order Date\": \"order date\",\n",
    "    \"Order ID\": \"order id\",\n",
    "    \"Ship Date\": \"ship date\",\n",
    "    \"Units Sold\": \"units sold\",\n",
    "    \"Unit Price\": \"unit price\",\n",
    "    \"Unit Cost\": \"unit cost\",\n",
    "    \"Total Revenue\": \"total revenue\",\n",
    "    \"Total Cost\": \"total cost\",\n",
    "    \"Total Profit\": \"total profit\",\n",
    "    }, \n",
    "    inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605c2be3-077b-4b8c-8882-6deac8a398da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump Dataframe metadata\n",
    "logger.info(f\"DataFrame shape: {data.shape}\")\n",
    "logger.info(\"\\nDataFrame info:\")\n",
    "logger.info(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43a504a-3657-4b56-b2f5-62c7a93d636f",
   "metadata": {},
   "source": [
    "Now that we have our data queried and available, let's prepare it for our machine learning models. We'll perform the following steps:\n",
    "\n",
    "1. Split the data into features (X) and target variable (y)\n",
    "2. Handle any missing values\n",
    "3. Encode categorical variables\n",
    "4. Scale numerical features\n",
    "5. Split the data into training and testing sets\n",
    "\n",
    "Let's start by preparing our feature matrix and target variable:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4367eb-6c81-4b70-ada8-edb8136dd71f",
   "metadata": {},
   "source": [
    "Amazon SageMaker's XGBoost container expects data in the libSVM or CSV data format.  For this example, we'll stick to CSV.  Note that the first column must be the target variable and the CSV should not include headers.  Also, notice that although repetitive it's easiest to do this after the train|validation|test split rather than before.  This avoids any misalignment issues due to random reordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c68276-84cc-42f2-bc80-24fc1305797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def process_data(data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Process and prepare data for modeling\n",
    "    \"\"\"\n",
    "    # Create copy to avoid modifying original\n",
    "    df = data.copy()\n",
    "    \n",
    "    # Drop 'order id' column\n",
    "    df = df.drop('order id', axis=1)\n",
    "    \n",
    "    # Convert date columns to datetime and extract features\n",
    "    date_columns = ['order date', 'ship date']\n",
    "    for col in date_columns:\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "        df[f'{col}_year'] = df[col].dt.year\n",
    "        df[f'{col}_month'] = df[col].dt.month\n",
    "        df[f'{col}_quarter'] = df[col].dt.quarter\n",
    "    \n",
    "    # Drop original date columns\n",
    "    df = df.drop(columns=date_columns)\n",
    "    \n",
    "    # Create lag features for 'total revenue'\n",
    "    for i in range(1, 4):\n",
    "        df[f'revenue_lag_{i}'] = df.groupby(['item type', 'sales channel'])['total revenue'].shift(i)\n",
    "    \n",
    "    # Drop rows with NaN values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Convert categorical variables to dummy variables\n",
    "    categorical_columns = ['region', 'country', 'item type', 'sales channel', 'order priority']\n",
    "    df_encoded = pd.get_dummies(df, columns=categorical_columns)\n",
    "    \n",
    "    # Prepare features and target\n",
    "    target_column = 'total profit'  # Assuming 'total profit' is the target variable\n",
    "    numeric_columns = ['units sold', 'unit price', 'unit cost', 'total revenue', 'total cost']\n",
    "    feature_columns = [col for col in df_encoded.columns if col != target_column]\n",
    "    X = df_encoded[feature_columns]\n",
    "    y = df_encoded[target_column].astype(float)\n",
    "    \n",
    "    # Train-test-validation split\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=1729)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.33, random_state=1729)\n",
    "    \n",
    "    # Scale numeric features\n",
    "    scaler = StandardScaler()\n",
    "    numeric_features = [col for col in X_train.columns if col in numeric_columns + \n",
    "                        ['order date_year', 'order date_month', 'order date_quarter',\n",
    "                         'ship date_year', 'ship date_month', 'ship date_quarter'] +\n",
    "                        [f'revenue_lag_{i}' for i in range(1, 4)]]\n",
    "    \n",
    "    X_train[numeric_features] = scaler.fit_transform(X_train[numeric_features])\n",
    "    X_val[numeric_features] = scaler.transform(X_val[numeric_features])\n",
    "    X_test[numeric_features] = scaler.transform(X_test[numeric_features])\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, feature_columns, scaler\n",
    "\n",
    "# Process the data\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, feature_columns, scaler = process_data(data)\n",
    "\n",
    "# Print some information about the processed data\n",
    "print(\"\\nProcessed data shape:\", X_train.shape)\n",
    "print(\"\\nFirst few rows of processed data:\")\n",
    "print(X_train.head())\n",
    "print(X_train.shape)\n",
    "print(X_train.info())\n",
    "print(\"\\nColumn names:\")\n",
    "print(X_train.columns.tolist())\n",
    "\n",
    "# Verify target variable\n",
    "print(\"\\nSummary statistics of the target variable:\")\n",
    "print(y_train.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dd59f1-27b1-4d25-8f55-8c8f3f422e51",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Training XGBoost\n",
    "\n",
    "### Option 1: Using the SageMaker Decorator\n",
    "Now we know that most of our features have skewed distributions, some are highly correlated with one another, and some appear to have non-linear relationships with our target variable.  Also, for targeting future prospects, good predictive accuracy is preferred to being able to explain why that prospect was targeted.  Taken together, these aspects make gradient boosted trees a good candidate algorithm.\n",
    "\n",
    "There are several intricacies to understanding the algorithm, but at a high level, gradient boosted trees works by combining predictions from many simple models, each of which tries to address the weaknesses of the previous models. By doing this the collection of simple models can actually outperform large, complex models.  Other Amazon SageMaker notebooks elaborate on gradient boosting trees further and how they differ from similar algorithms.\n",
    "\n",
    "`xgboost` is an extremely popular, open-source package for gradient boosted trees.  It is computationally powerful, fully featured, and has been successfully used in many machine learning competitions.  \n",
    "\n",
    "Let's train a first version of XGBoost using this open-source library and using SageMaker's @remote decorator. You can use the @remote decorator to annotate a function. SageMaker AI will transform the code inside the decorator into a SageMaker training job. \n",
    "\n",
    "Note how we log various parameters, metrics, tags, and artifacts to MLflow. When the training is finished, don't forget to open up MLflow to take a look at the experiment results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569c1c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import os\n",
    "import joblib\n",
    "from sagemaker.remote_function import remote\n",
    "\n",
    "\n",
    "def train_model(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Train XGBoost model\n",
    "    \"\"\"\n",
    "    # Initialize model\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "@remote(job_name_prefix=\"xgboost-sales-forecast\", \n",
    "        instance_type=\"ml.m5.large\", \n",
    "        keep_alive_period_in_seconds=600,)\n",
    "def model_train(X_train, y_train, X_val, y_val, mlflow_arn):\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the model training process\n",
    "    \"\"\"\n",
    "    mlflow.set_tracking_uri(mlflow_arn)\n",
    "    mlflow.set_experiment(\"XG-Boost\")\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"xgboost-decorator-{datetime.now(timezone.utc).strftime('%Y%m%d%H%M%S')}\"):\n",
    "        # Log information about the data\n",
    "        mlflow.log_param(\"train_samples\", len(X_train))\n",
    "        mlflow.log_param(\"val_samples\", len(X_val))\n",
    "        mlflow.log_param(\"features\", X_train.shape[1])\n",
    "        \n",
    "        # Train model\n",
    "        model = train_model(X_train, y_train, X_val, y_val)\n",
    "        \n",
    "        # Log model parameters\n",
    "        params = model.get_params()\n",
    "        mlflow.log_params(params)\n",
    "        \n",
    "        # Log validation results\n",
    "        results = model.evals_result()\n",
    "        for epoch, rmse_value in enumerate(results['validation_0']['rmse']):\n",
    "            mlflow.log_metric('train_rmse', rmse_value, step=epoch)\n",
    "\n",
    "        # Log final metrics\n",
    "        final_rmse = results['validation_0']['rmse'][-1]\n",
    "        best_rmse = min(results['validation_0']['rmse'])\n",
    "        best_epoch = results['validation_0']['rmse'].index(best_rmse)\n",
    "        mlflow.log_metrics({\n",
    "            'final_rmse': final_rmse,\n",
    "            'best_rmse': best_rmse,\n",
    "            'best_epoch': best_epoch\n",
    "        })\n",
    "\n",
    "        # Set tags for the run\n",
    "        mlflow.set_tag(\"model_type\", \"XGBoost\")\n",
    "        mlflow.set_tag(\"framework\", \"OSS\")\n",
    "        \n",
    "        # Infer model signature and register model\n",
    "        predictions = model.predict(X_val)\n",
    "        signature = mlflow.models.infer_signature(X_train, predictions)\n",
    "        mlflow.xgboost.log_model(model, \"model\", registered_model_name=\"xgboost-lib-regression\", signature=signature)\n",
    "        \n",
    "        # Save model\n",
    "        path = \"/opt/ml/model\"\n",
    "        joblib.dump(model, os.path.join(path, 'revenue_forecast_model.joblib'))\n",
    "    return model, predictions\n",
    "\n",
    "# Run the training\n",
    "xgb_model, output = model_train(X_train, y_train, X_val, y_val, mlflow_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9f781a",
   "metadata": {},
   "source": [
    "### Option 2: Using SageMaker's built-in algorithm\n",
    "\n",
    "Amazon SageMaker also has a managed, distributed [training framework for XGBoost](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html). This section shows how you can use train this version of XGBoost. Instead of using the @remote decorator, this section shows how to use the [ModelTrainer](https://sagemaker.readthedocs.io/en/stable/api/training/model_trainer.html) SDK to create a training job.\n",
    "\n",
    "First we must adjust the data to be suitable for this version of XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9f7f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_for_sagemaker_xgboost(X, y, filename):\n",
    "    \"\"\"\n",
    "    Save data in a format compatible with SageMaker's XGBoost algorithm.\n",
    "    \"\"\"\n",
    "    # Combine target and features\n",
    "    data = pd.concat([y.reset_index(drop=True), X.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    # Convert boolean columns to int\n",
    "    bool_columns = data.select_dtypes(include=['bool']).columns\n",
    "    data[bool_columns] = data[bool_columns].astype(int)\n",
    "    \n",
    "    # Ensure all data is numeric\n",
    "    data = data.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Replace any remaining non-numeric values with 0\n",
    "    data = data.fillna(0)\n",
    "    \n",
    "    # Save to csv without header and index\n",
    "    data.to_csv(filename, header=False, index=False)\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "# Combine train and validation sets\n",
    "X_train_full = pd.concat([X_train, X_val])\n",
    "y_train_full = pd.concat([y_train, y_val])\n",
    "\n",
    "# Save training data (including validation data)\n",
    "save_for_sagemaker_xgboost(X_train_full, y_train_full, 'train.csv')\n",
    "\n",
    "# Save test data\n",
    "save_for_sagemaker_xgboost(X_test, y_test, 'test.csv')\n",
    "\n",
    "# Print some information about the saved files\n",
    "print(\"\\nTrain file info:\")\n",
    "print(pd.read_csv('train.csv', header=None).info())\n",
    "\n",
    "print(\"\\nTest file info:\")\n",
    "print(pd.read_csv('test.csv', header=None).info())\n",
    "\n",
    "# Verify first few rows of each file\n",
    "print(\"\\nFirst few rows of train.csv:\")\n",
    "print(pd.read_csv('train.csv', header=None).head())\n",
    "\n",
    "print(\"\\nFirst few rows of test.csv:\")\n",
    "print(pd.read_csv('test.csv', header=None).head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b83856-154e-4789-add8-db58b27e41a6",
   "metadata": {},
   "source": [
    "In the cell above, we performed preprocessing on our dataset and produced output files \"train.csv\" and \"test.csv\". Next, we'll upload these local files to our S3 location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da67f22f-ff96-4609-a197-68abc2a18877",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train_xgboost/train.csv')).upload_file('train.csv')\n",
    "session.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'test_xgboost/test.csv')).upload_file('test.csv')\n",
    "session.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation_xgboost/test.csv')).upload_file('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0e05af-81dc-472f-9f57-3d0fb85f663b",
   "metadata": {},
   "source": [
    "We'll need to specify the ECR container location for Amazon SageMaker's implementation of XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fd0ef4-388a-4354-94b6-396cecbe34cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = sagemaker.image_uris.retrieve(region=boto3.Session().region_name, framework='xgboost', version='latest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e92e1f5-905c-4202-993f-b9d07f24f33c",
   "metadata": {},
   "source": [
    "Then, because we're training with the CSV file format, we'll create `TrainingInput` objects that our training function can use as a pointer to the files in S3, which also specify that the content type is CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64769cd-f2ff-4c1c-94f5-e488f8c649fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.inputs.TrainingInput(\n",
    "    s3_data='s3://{}/{}/train_xgboost/'.format(bucket, prefix),\n",
    "    content_type='csv'\n",
    ")\n",
    "s3_input_test = sagemaker.inputs.TrainingInput(\n",
    "    s3_data='s3://{}/{}/test_xgboost/'.format(bucket, prefix),\n",
    "    content_type='csv'\n",
    ")\n",
    "s3_input_validation = sagemaker.inputs.TrainingInput(\n",
    "    s3_data='s3://{}/{}/validation_xgboost/'.format(bucket, prefix),\n",
    "    content_type='csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4db3d1-a6e8-46ea-9a6e-35339e4b9ef5",
   "metadata": {},
   "source": [
    "First we'll need to specify training parameters to the estimator.  This includes:\n",
    "1. The `xgboost` algorithm container\n",
    "1. The IAM role to use\n",
    "1. Training instance type and count\n",
    "1. S3 location for output data\n",
    "1. Algorithm hyperparameters\n",
    "\n",
    "And then a `.fit()` function which specifies the input data.  In this case we have both a training and validation set which are passed in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003300a9-c6c3-4459-bedc-c9697da03940",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.models import infer_signature\n",
    "\n",
    "sm_session = sagemaker.Session()\n",
    "xgb_estimator = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    instance_count=1, \n",
    "                                    instance_type='ml.m5.xlarge',\n",
    "                                    output_path=f's3://{bucket}/{prefix}/output',\n",
    "                                    sagemaker_session=sm_session)\n",
    "\n",
    "hyperparameters = {\n",
    "    \"max_depth\": 6,\n",
    "    \"eta\": 0.2,\n",
    "    \"gamma\": 4,\n",
    "    \"min_child_weight\": 8,\n",
    "    \"subsample\": 0.6,\n",
    "    \"verbosity\": 0,\n",
    "    \"objective\": \"reg:linear\",\n",
    "    \"num_round\": 75,\n",
    "}\n",
    "xgb_estimator.set_hyperparameters(**hyperparameters)\n",
    "\n",
    "mlflow.set_experiment(\"XG-Boost\")\n",
    "with mlflow.start_run(run_name=f\"xgboost-builtin-{datetime.now(timezone.utc).strftime('%Y%m%d%H%M%S')}\"):\n",
    "    # Log the hyperparameters\n",
    "    mlflow.log_params(hyperparameters)\n",
    "\n",
    "    # Fit the model and capture training metrics\n",
    "    xgb_estimator.fit({'train': s3_input_train, 'validation': s3_input_test})\n",
    "    \n",
    "    # Get the training job name\n",
    "    job_name = xgb_estimator.latest_training_job.job_name\n",
    "    \n",
    "    # Get the training job description\n",
    "    client = sm_session.boto_session.client('sagemaker')\n",
    "    training_job_description = client.describe_training_job(TrainingJobName=job_name)\n",
    "    \n",
    "    # Extract and log metrics\n",
    "    for metric in training_job_description['FinalMetricDataList']:\n",
    "        metric_name = metric['MetricName']\n",
    "        metric_value = metric['Value']\n",
    "        mlflow.log_metric(metric_name, metric_value)\n",
    "\n",
    "    # Set tags for the run\n",
    "    mlflow.set_tag(\"model_type\", \"XGBoost\")\n",
    "    mlflow.set_tag(\"framework\", \"SageMaker\")\n",
    "\n",
    "    # Register the model\n",
    "    mlflow.register_model(f\"runs:/{mlflow.active_run().info.run_id}/model\", \"xgboost-sm-regression\")\n",
    "\n",
    "    print(f\"Model saved in run {mlflow.active_run().info.run_uuid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bfccde-d940-424e-903c-bf1122c38714",
   "metadata": {},
   "source": [
    "Now that you have successfully completed the training of the XGBoost model and SageMaker Autopilot job on the dataset, you can deploy xgboost and create a model from any of the candidates by using [Inference Pipelines](https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipelines.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693f1424-7cac-411f-82f0-836630ba6bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor = xgb_estimator.deploy(initial_instance_count=1,\n",
    "                           instance_type='ml.m5.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022d9db7-594f-48f1-84e8-b5123d4aede3",
   "metadata": {},
   "source": [
    "First we'll need to determine how we pass data into and receive data from our endpoint.  Our data is currently stored as NumPy arrays in memory of our notebook instance.  To send it in an HTTP POST request, we'll serialize it as a CSV string and then decode the resulting CSV.\n",
    "\n",
    "*Note: For inference with CSV format, SageMaker XGBoost requires that the data does NOT include the target variable.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225ae8c7-e079-48e6-9672-f5252182a4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.serializer = sagemaker.serializers.CSVSerializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89f8de3-4bb4-4c93-9c03-414506a4366f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T19:29:20.445680Z",
     "iopub.status.busy": "2025-01-06T19:29:20.444866Z",
     "iopub.status.idle": "2025-01-06T19:29:20.487983Z",
     "shell.execute_reply": "2025-01-06T19:29:20.486542Z",
     "shell.execute_reply.started": "2025-01-06T19:29:20.445642Z"
    }
   },
   "source": [
    "Now, we'll use a simple function to:\n",
    "1. Loop over our test dataset\n",
    "1. Split it into mini-batches of rows \n",
    "1. Convert those mini-batches to CSV string payloads (notice, we drop the target variable from our dataset first)\n",
    "1. Retrieve mini-batch predictions by invoking the XGBoost endpoint\n",
    "1. Collect predictions and convert from the CSV output our model provides into a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb267f7-8ca8-4d33-afac-52aebf6d2fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "def predict(data, predictor, rows=500):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = []\n",
    "    \n",
    "    for array in split_array:\n",
    "        # Convert numpy array to CSV string\n",
    "        csv = '\\n'.join([','.join(map(str, row)) for row in array])\n",
    "        \n",
    "        # Get predictions\n",
    "        prediction = predictor.predict(csv)\n",
    "        \n",
    "        # Decode and convert to numpy array\n",
    "        prediction_array = np.fromstring(prediction.decode('utf-8'), sep=',')\n",
    "        \n",
    "        predictions.append(prediction_array)\n",
    "    \n",
    "    # Concatenate all predictions\n",
    "    return np.concatenate(predictions)\n",
    "\n",
    "# Print column names and data info for debugging\n",
    "print(\"Original columns:\", test_data.columns)\n",
    "print(test_data.info())\n",
    "\n",
    "# Remove the first column (target variable)\n",
    "X_test = test_data.iloc[:, 1:]\n",
    "\n",
    "# Print shape to confirm\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Columns of X_test:\", X_test.columns)\n",
    "\n",
    "# Ensure we have the correct number of features\n",
    "if X_test.shape[1] != 224:\n",
    "    print(f\"Warning: You have {X_test.shape[1]} features. The model expects 224.\")\n",
    "    print(\"Current features:\", X_test.columns.tolist())\n",
    "    # If needed, you can manually select the correct 8 features:\n",
    "    # X_test = X_test[['feature1', 'feature2', ..., 'feature8']]\n",
    "\n",
    "# Now you can use X_test in your predict function\n",
    "predictions = predict(X_test.values, xgb_predictor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5868b0c-991d-4519-b899-ab43f30a7b3c",
   "metadata": {},
   "source": [
    "Now we'll output a score for the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3717de0-f579-4051-b83f-d7c94623af6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Extract the actual values (first column)\n",
    "y_true = test_data.iloc[:, 0]\n",
    "\n",
    "# Ensure predictions are in the same format as y_true\n",
    "y_pred = predictions  # This should be the predictions from your previous cell\n",
    "\n",
    "# Make sure y_true and y_pred have the same length\n",
    "assert len(y_true) == len(y_pred), \"Mismatch in length between actual and predicted values\"\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:.4f}\")\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"R-squared Score: {r2:.4f}\")\n",
    "\n",
    "# If you want to see a sample of the actual vs predicted values\n",
    "comparison_df = pd.DataFrame({'Actual': y_true, 'Predicted': y_pred})\n",
    "print(\"\\nSample of Actual vs Predicted values:\")\n",
    "print(comparison_df.head(10))\n",
    "\n",
    "# If you want to plot the actual vs predicted values\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_true, y_pred, alpha=0.5)\n",
    "plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.show()\n",
    "\n",
    "# Calculate and print additional statistics\n",
    "print(\"\\nAdditional Statistics:\")\n",
    "print(f\"Mean of Actual Values: {y_true.mean():.4f}\")\n",
    "print(f\"Mean of Predicted Values: {y_pred.mean():.4f}\")\n",
    "print(f\"Standard Deviation of Actual Values: {y_true.std():.4f}\")\n",
    "print(f\"Standard Deviation of Predicted Values: {y_pred.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a4699d-d734-447e-9b87-23d0c33df7b3",
   "metadata": {},
   "source": [
    "#### Optional: Hyperparameter Tuning Job\n",
    "\n",
    "We can optionally run a Hyperparameter Optimization (HPO) Job to improve our results. This will run multiple training jobs with different values for the hyperparameters based on the ranges specified below. The HPO job automatically chooses new parameter values based on the results of previous jobs, eventually leading to better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc4dd4b-440c-4e2b-9a14-1d3b929fc70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import HyperparameterTuner\n",
    "from sagemaker.parameter import ContinuousParameter, IntegerParameter\n",
    "\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    \"max_depth\": IntegerParameter(5, 10),\n",
    "    \"eta\": ContinuousParameter(0.001, 0.3),\n",
    "    \"min_child_weight\": IntegerParameter(5, 10),\n",
    "    \"subsample\": ContinuousParameter(0.3, 0.8),\n",
    "    \"num_round\": IntegerParameter(50, 100),\n",
    "}\n",
    "\n",
    "objective_metric_name = \"validation:rmse\"\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    xgb_estimator,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    objective_type=\"Minimize\",\n",
    "    strategy=\"Bayesian\",\n",
    "    max_jobs=10,\n",
    "    max_parallel_jobs=5,\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4075b5-df7b-4666-86f8-3f44f42aeadc",
   "metadata": {},
   "source": [
    "When we run the HPO job, we can log the results of the hyperparameter tuning job and each child training job into MLflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90281c8-a5b7-47f7-bf1d-884eaf742045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_param_range(param_range):\n",
    "    formatted_param_range = {\n",
    "        f\"{param_range['Name']}_min_value\": param_range['MinValue'],\n",
    "        f\"{param_range['Name']}_max_value\": param_range['MaxValue']\n",
    "    }\n",
    "    return formatted_param_range\n",
    "\n",
    "\n",
    "mlflow.set_experiment(\"XG-Boost-HPO\")\n",
    "with mlflow.start_run(run_name=f\"xgboost-hpo-{datetime.now(timezone.utc).strftime('%Y%m%d%H%M%S')}\"):\n",
    "    tuner.fit({\"train\": s3_input_train, \"validation\": s3_input_validation})\n",
    "    tuner_descr = tuner.describe()\n",
    "    \n",
    "    # Log parameters relevant to overall tuning job\n",
    "    mlflow.log_params(tuner_descr['HyperParameterTuningJobConfig']['ResourceLimits'])\n",
    "    mlflow.log_param('Strategy', tuner_descr['HyperParameterTuningJobConfig']['Strategy'])\n",
    "    mlflow.log_params(tuner_descr['TrainingJobDefinition']['StaticHyperParameters'])\n",
    "    for integer_param in tuner_descr['HyperParameterTuningJobConfig']['ParameterRanges']['IntegerParameterRanges']:\n",
    "        mlflow.log_params(format_param_range(integer_param))\n",
    "    for continuous_param in tuner_descr['HyperParameterTuningJobConfig']['ParameterRanges']['ContinuousParameterRanges']:\n",
    "        mlflow.log_params(format_param_range(continuous_param))\n",
    "    for categorical_param in tuner_descr['HyperParameterTuningJobConfig']['ParameterRanges']['CategoricalParameterRanges']:\n",
    "        mlflow.log_params(format_param_range(categorical_param))\n",
    "    mlflow.log_param('BestTrainingJobName', tuner_descr['BestTrainingJob']['TrainingJobName'])\n",
    "\n",
    "    # Set tags for the run\n",
    "    mlflow.set_tag(\"model_type\", \"XGBoost\")\n",
    "    mlflow.set_tag(\"framework\", \"SageMaker\")\n",
    "\n",
    "    # Log parameters and metrics for each training job\n",
    "    train_summaries = tuner.analytics().training_job_summaries()\n",
    "    for train_results in train_summaries:\n",
    "        with mlflow.start_run(run_name=train_results['TrainingJobName'], nested=True):\n",
    "            mlflow.log_params(train_results['TunedHyperParameters'])\n",
    "            mlflow.log_metric(train_results['FinalHyperParameterTuningJobObjectiveMetric']['MetricName'],\n",
    "                             train_results['FinalHyperParameterTuningJobObjectiveMetric']['Value'])\n",
    "            mlflow.set_tag(\"model_type\", \"XGBoost\")\n",
    "            mlflow.set_tag(\"framework\", \"SageMaker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb14d426-4ba2-4525-ae60-25124ca22a43",
   "metadata": {},
   "source": [
    "It is also easy to deploy the best model from the hyperparameter tuning job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c816a536-bc81-4c2f-a166-d450e0f270dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.deploy(\n",
    "    initial_instance_count=1, \n",
    "    instance_type='ml.m5.xlarge'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af854c3c-7b28-440d-bfb9-601307e11a6d",
   "metadata": {},
   "source": [
    "---\n",
    "## AutoML Training\n",
    "\n",
    "Amazon SageMaker Autopilot is an automated machine learning (commonly referred to as AutoML) solution for tabular datasets. You can use SageMaker Autopilot in different ways: on autopilot (hence the name) or with human guidance, without code through SageMaker Studio, or using the AWS SDKs. This notebook, as a first glimpse, will use the AWS SDKs to simply create and deploy a machine learning model.\n",
    "\n",
    "This part of the notebook demonstrates how you can use Autopilot on this dataset to get the most accurate ML pipeline through exploring a number of potential options, or \"candidates\". Each candidate generated by Autopilot consists of two steps. The first step performs automated feature engineering on the dataset and the second step trains and tunes an algorithm to produce a model. When you deploy this model, it follows similar steps. Feature engineering followed by inference, to decide whether the lead is worth pursuing or not. The notebook contains instructions on how to train the model as well as to deploy the model to perform batch predictions on a set of leads. Where it is possible, use the Amazon SageMaker Python SDK, a high level SDK, to simplify the way you interact with Amazon SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8d2871-85a2-42cc-b07b-ed1837ce96a6",
   "metadata": {},
   "source": [
    "First, we need to upload the entire dataset to S3.\n",
    "\n",
    "Caution: Before running the cell below, you must upload the data file \"5000-sales-records.csv\" to the local directory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5866a843-88d1-4206-b05c-11ebca7972b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data for AutoML Job\n",
    "\n",
    "session.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train_automl/train.csv')).upload_file('5000-sales-records.csv')\n",
    "session.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'test_automl/test.csv')).upload_file('5000-sales-records.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a687e5c8-c205-4884-bd9e-785045258cb5",
   "metadata": {},
   "source": [
    "### AutoML Configuration\n",
    "\n",
    "You can specify the type of problem you want to solve with your dataset (`Regression, MulticlassClassification, BinaryClassification`). In case you are not sure, SageMaker Autopilot will infer the problem type based on statistics of the target column (the column you want to predict). \n",
    "\n",
    "You have the option to limit the running time of a SageMaker Autopilot job by providing either the maximum number of pipeline evaluations or candidates (one pipeline evaluation is called a `Candidate` because it generates a candidate model) or providing the total time allocated for the overall Autopilot job. Under default settings, this job takes about four hours to run. This varies between runs because of the nature of the exploratory process Autopilot uses to find optimal training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c0bed7-21d6-48cb-9584-3e9f5c899304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime, sleep\n",
    "import json\n",
    "import mlflow\n",
    "import boto3\n",
    "\n",
    "# Set up MLflow experiment\n",
    "mlflow.set_experiment(\"AutoML-Job\")\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run(run_name=\"AutoML-Job-Run\"):\n",
    "\n",
    "    input_data_config = [{\n",
    "        'DataSource': {\n",
    "            'S3DataSource': {\n",
    "                'S3DataType': 'S3Prefix',\n",
    "                'S3Uri': f's3://{bucket}/{prefix}/train_automl'\n",
    "            }\n",
    "        },\n",
    "        'ContentType': 'text/csv;header=present',\n",
    "        'TargetAttributeName': 'Total Profit'\n",
    "    }]\n",
    "\n",
    "    output_data_config = {\n",
    "        'S3OutputPath': f's3://{bucket}/{prefix}/output_automl'\n",
    "    }\n",
    "\n",
    "    auto_ml_job_config = {\n",
    "        'CompletionCriteria': {\n",
    "            'MaxCandidates': 5\n",
    "        }\n",
    "    }\n",
    "\n",
    "    autoMLJobObjective = {\n",
    "        \"MetricName\": \"MSE\"  \n",
    "    }\n",
    "\n",
    "    # Log configurations to MLflow\n",
    "    mlflow.log_dict(input_data_config, \"input_data_config.json\")\n",
    "    mlflow.log_dict(output_data_config, \"output_data_config.json\")\n",
    "    mlflow.log_dict(auto_ml_job_config, \"auto_ml_job_config.json\")\n",
    "    mlflow.log_dict(autoMLJobObjective, \"autoMLJobObjective.json\")\n",
    "\n",
    "    # Configuration\n",
    "    timestamp_suffix = strftime('%d-%H-%M-%S', gmtime())\n",
    "    auto_ml_job_name = 'demo' + timestamp_suffix\n",
    "    print('AutoMLJobName: ' + auto_ml_job_name)\n",
    "    mlflow.log_param(\"AutoMLJobName\", auto_ml_job_name)\n",
    "\n",
    "    # Create AutoML job\n",
    "    sm = boto3.client('sagemaker')\n",
    "    sm.create_auto_ml_job(\n",
    "        AutoMLJobName=auto_ml_job_name,\n",
    "        InputDataConfig=input_data_config,\n",
    "        OutputDataConfig=output_data_config,\n",
    "        AutoMLJobConfig=auto_ml_job_config,\n",
    "        AutoMLJobObjective=autoMLJobObjective,\n",
    "        ProblemType=\"Regression\",  \n",
    "        RoleArn=role  \n",
    "    )\n",
    "\n",
    "    # Wait for the AutoML job to complete\n",
    "    while True:\n",
    "        response = sm.describe_auto_ml_job(AutoMLJobName=auto_ml_job_name)\n",
    "        status = response['AutoMLJobStatus']\n",
    "        if status in ['Completed', 'Failed', 'Stopped']:\n",
    "            break\n",
    "        print(f\"AutoML job status: {status}\")\n",
    "        sleep(60)\n",
    "\n",
    "    # Log final job status\n",
    "    mlflow.log_param(\"FinalJobStatus\", status)\n",
    "\n",
    "    if status == 'Completed':\n",
    "        # Log best candidate info\n",
    "        best_candidate = response['BestCandidate']\n",
    "        mlflow.log_dict(best_candidate, \"best_candidate.json\")\n",
    "        \n",
    "        # Log objective metric\n",
    "        objective_metric = best_candidate['FinalAutoMLJobObjectiveMetric']\n",
    "        mlflow.log_metric(objective_metric['MetricName'], objective_metric['Value'])\n",
    "\n",
    "        # Log other metrics if available\n",
    "        if 'CandidateProperties' in best_candidate:\n",
    "            for metric in best_candidate['CandidateProperties'].get('Metrics', []):\n",
    "                mlflow.log_metric(metric['MetricName'], metric['Value'])\n",
    "\n",
    "    print(f\"AutoML job {auto_ml_job_name} finished with status: {status}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c7bac7-f750-4263-aea7-73a8e2d17a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "best_candidate = sagemaker_client.describe_auto_ml_job(AutoMLJobName=auto_ml_job_name)['BestCandidate']\n",
    "best_candidate_name = best_candidate['CandidateName']\n",
    "print(best_candidate)\n",
    "print('\\n')\n",
    "\n",
    "print(\"CandidateName: \" + best_candidate_name)\n",
    "print(\"FinalAutoMLJobObjectiveMetricName: \" + best_candidate['FinalAutoMLJobObjectiveMetric']['MetricName'])\n",
    "print(\"FinalAutoMLJobObjectiveMetricValue: \" + str(best_candidate['FinalAutoMLJobObjectiveMetric']['Value']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956b70d5",
   "metadata": {},
   "source": [
    "### Create Model for best candidate\n",
    "\n",
    "When the AutoML job has finished running, you can easily create a SageMaker Model object using the best candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f69a99-7d29-426b-a147-db6222ebbfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "timestamp_suffix = strftime('%d-%H-%M-%S', gmtime())\n",
    "\n",
    "model_name = 'demo-' + timestamp_suffix\n",
    "print(f\"Model name: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1167ba0b-5f3c-4b70-8692-ba5a567c9da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model\n",
    "model = sagemaker_client.create_model(\n",
    "    Containers=best_candidate['InferenceContainers'],\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role\n",
    ")\n",
    "\n",
    "print('Model ARN corresponding to the best candidate is : {}'.format(model['ModelArn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b99203-019b-4c16-aca5-856e6751c192",
   "metadata": {},
   "source": [
    "### View other candidates\n",
    "You can view all the candidates (pipeline evaluations with different hyperparameter combinations) that were explored by SageMaker Autopilot and sort them by their final performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8d4b63-4ef1-4f53-899e-69882a98ca07",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = sagemaker_client.list_candidates_for_auto_ml_job(\n",
    "    AutoMLJobName=auto_ml_job_name, SortBy='FinalObjectiveMetricValue')['Candidates']\n",
    "\n",
    "index = 1\n",
    "for candidate in candidates:\n",
    "  print (str(index) + \"  \" + candidate['CandidateName'] + \"  \" + str(candidate['FinalAutoMLJobObjectiveMetric']['Value']))\n",
    "  index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac7ef2d-0bb3-4a59-a3e7-bfaef07757ec",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "The Autopilot job creates many underlying artifacts such as dataset splits, preprocessing scripts, or preprocessed data, etc. This code, when un-commented, deletes them. This operation deletes all the generated models and the auto-generated notebooks as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ba43ea-6c41-4a60-8730-ff4327e35428",
   "metadata": {},
   "outputs": [],
   "source": [
    "#s3 = boto3.resource('s3')\n",
    "#bucket = s3.Bucket(bucket)\n",
    "\n",
    "#job_outputs_prefix = '{}/output/{}'.format(prefix,auto_ml_job_name)\n",
    "#bucket.objects.filter(Prefix=job_outputs_prefix).delete()\n",
    "# xgb_predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "# tuner.delete_endpoint(delete_endpoint_config=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
