{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve current AWS account and Region"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In this cell, we are using boto3 library to get the current AWS account and AWS region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T19:00:20.799625Z",
     "iopub.status.busy": "2025-04-01T19:00:20.798801Z",
     "iopub.status.idle": "2025-04-01T19:00:28.470427Z",
     "shell.execute_reply": "2025-04-01T19:00:28.469396Z",
     "shell.execute_reply.started": "2025-04-01T19:00:20.799590Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current AWS Account ID: 975050082261\n",
      "Current region : us-west-2\n",
      "Connection: project.iam | Run start time: 2025-04-01 19:00:22.668306 | Run duration : 0:00:05.798763s.\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "import boto3\n",
    "sts_client = boto3.client('sts')\n",
    "account_id = sts_client.get_caller_identity()['Account']\n",
    "current_region = sts_client.meta.region_name\n",
    "\n",
    "print(f\"Current AWS Account ID: {account_id}\")\n",
    "print(f\"Current region : {current_region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T17:37:03.385236Z",
     "iopub.status.busy": "2025-01-07T17:37:03.384649Z",
     "iopub.status.idle": "2025-01-07T17:37:03.389036Z",
     "shell.execute_reply": "2025-01-07T17:37:03.387994Z",
     "shell.execute_reply.started": "2025-01-07T17:37:03.385209Z"
    }
   },
   "source": [
    "## Configure our Spark Session to use a RMS backed catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code initializes a PySpark session with specific configurations for working with Apache Iceberg tables through AWS Glue catalog. It sets up:\n",
    "\n",
    "1. A custom catalog named 'salesrep'\n",
    "2. AWS Glue as the catalog implementation\n",
    "3. Iceberg extensions for Spark\n",
    "The session is configured to use Iceberg as the table format and AWS Glue as the metadata store, enabling data lake operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T19:03:43.053262Z",
     "iopub.status.busy": "2025-04-01T19:03:43.052799Z",
     "iopub.status.idle": "2025-04-01T19:05:08.238799Z",
     "shell.execute_reply": "2025-04-01T19:05:08.238035Z",
     "shell.execute_reply.started": "2025-04-01T19:03:43.053233Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing for connection type: SPARK_GLUE, connection name: project.spark.compatibility\n",
      "Creating Glue session...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Session 484mug4zuyzq4w-8e35c621-1225-4391-a778-19916c42ca01 has been created.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <table class=\"session_info_table\" style=\"width: 75%; margin-top: var(--jp-content-heading-margin-top); margin-bottom:var(--jp-content-heading-margin-bottom); border: var(--jp-border-width) solid var(--jp-border-color2);\">\n",
       "                    <tr>\n",
       "                        <th style=\"text-align: left; border: var(--jp-border-width) solid var(--jp-border-color2);\">Id</th>\n",
       "                        <th style=\"text-align: left; border: var(--jp-border-width) solid var(--jp-border-color2);\">Spark UI</th>\n",
       "                        <th style=\"text-align: left; border: var(--jp-border-width) solid var(--jp-border-color2);\">Driver logs</th>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td class=\"application_id\" style=\"word-wrap: break-word; text-align: left; border: var(--jp-border-width) solid var(--jp-border-color2)\">484mug4zuyzq4w-8e35c621-1225-4391-a778-19916c42ca01</td>\n",
       "                        <td class=\"spark_ui_link\" style=\"word-wrap: break-word; text-align: left; border: var(--jp-border-width) solid var(--jp-border-color2)\"><a href=\"\" target=\"_blank\" log_location=\"s3://amazon-sagemaker-975050082261-us-west-2-73f007cc13b6/dzd_4ozgwmo60264i8/484mug4zuyzq4w/dev/glue/glue-spark-events-logs/\">link</a></td>\n",
       "                        <td class=\"driver_log_link\" style=\"word-wrap: break-word; text-align: left; border: var(--jp-border-width) solid var(--jp-border-color2)\"><a href=\"\" target=\"_blank\" log_location=\"s3://amazon-sagemaker-975050082261-us-west-2-73f007cc13b6/dzd_4ozgwmo60264i8/484mug4zuyzq4w/dev/glue/glue-spark-system-logs/484mug4zuyzq4w-8e35c621-1225-4391-a778-19916c42ca01/driver/stderr.gz\">link</a></td>\n",
       "                    </tr>\n",
       "                </table>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session created for connection: project.spark.compatibility.\n",
      "\n",
      "Connection: project.spark.compatibility | Run start time: 2025-04-01 19:03:43.558807 | Run duration : 0:01:24.677047s.\n"
     ]
    }
   ],
   "source": [
    "%%pyspark project.spark.compatibility\n",
    "from pyspark.sql import SparkSession\n",
    "import logging\n",
    "\n",
    "spark = SparkSession.builder.appName('rms_salesrep') \\\n",
    ".config('spark.sql.catalog.salesrep', 'org.apache.iceberg.spark.SparkCatalog') \\\n",
    ".config('spark.sql.defaultCatalog', 'salesrep') \\\n",
    ".config('spark.sql.catalog.salesrep.catalog-impl', 'org.apache.iceberg.aws.glue.GlueCatalog') \\\n",
    ".config('spark.sql.catalog.salesrep.glue.id','<AWS_ACCOUNT_ID>:salesrep-managed-rmscatalog/dev') \\\n",
    ".config('spark.sql.catalog.salesrep.client.region','<AWS_REGION>') \\\n",
    ".config('spark.sql.extensions','org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions') \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's configure the spark session to use the public namespace/database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T19:09:32.843337Z",
     "iopub.status.busy": "2025-04-01T19:09:32.840029Z",
     "iopub.status.idle": "2025-04-01T19:09:53.875909Z",
     "shell.execute_reply": "2025-04-01T19:09:53.875153Z",
     "shell.execute_reply.started": "2025-04-01T19:09:32.843291Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame[]\n",
      "Connection: project.spark.compatibility | Run start time: 2025-04-01 19:09:33.529532 | Run duration : 0:00:20.343055s.\n"
     ]
    }
   ],
   "source": [
    "%%pyspark project.spark.compatibility\n",
    "spark.sql(\"use public\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataframe from the csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code sets up a data processing pipeline using PySpark. It defines a structured schema for a sales dataset. The code then reads a CSV file from an S3 location using this schema, handles date formatting, and loads it into a DataFrame. It performs basic data exploration by displaying sample rows, verifying the schema structure, and calculating statistical summaries of numerical columns. The code also counts the total number of records and creates a temporary SQL view named \"view_salesrep\" for potential SQL queries. This setup enables further analysis of sales representative performance data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T19:10:15.860087Z",
     "iopub.status.busy": "2025-04-01T19:10:15.859736Z",
     "iopub.status.idle": "2025-04-01T19:10:34.526779Z",
     "shell.execute_reply": "2025-04-01T19:10:34.524442Z",
     "shell.execute_reply.started": "2025-04-01T19:10:15.860061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+------------+---------------------------+\n",
      "|      date|sales_rep_id|total_sales|deals_closed|customer_satisfaction_score|\n",
      "+----------+------------+-----------+------------+---------------------------+\n",
      "|2023-01-17|     REP-332|  218797.71|           4|                        4.6|\n",
      "|2023-02-16|     REP-816|  441543.91|          43|                        2.9|\n",
      "|2023-02-06|     REP-918|  462372.08|          34|                        4.4|\n",
      "|2022-08-20|     REP-567|  383212.67|          49|                        4.4|\n",
      "|2023-02-10|     REP-530|  437005.54|          14|                        4.6|\n",
      "+----------+------------+-----------+------------+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- sales_rep_id: string (nullable = true)\n",
      " |-- total_sales: double (nullable = true)\n",
      " |-- deals_closed: integer (nullable = true)\n",
      " |-- customer_satisfaction_score: double (nullable = true)\n",
      "\n",
      "+-------+------------+------------------+------------------+---------------------------+\n",
      "|summary|sales_rep_id|       total_sales|      deals_closed|customer_satisfaction_score|\n",
      "+-------+------------+------------------+------------------+---------------------------+\n",
      "|  count|        5000|              5000|              5000|                       5000|\n",
      "|   mean|        NULL|251217.36562800044|           25.8524|          3.004939999999997|\n",
      "| stddev|        NULL| 143725.1709545376|14.563579017767623|          1.138603038717163|\n",
      "|    min|     REP-100|           5101.59|                 1|                        1.0|\n",
      "|    max|     REP-999|         499905.71|                50|                        5.0|\n",
      "+-------+------------+------------------+------------------+---------------------------+\n",
      "\n",
      "Total number of records: 5000\n",
      "\n",
      "Connection: project.spark.compatibility | Run start time: 2025-04-01 19:10:16.573111 | Run duration : 0:00:17.947663s.\n"
     ]
    }
   ],
   "source": [
    "%%pyspark project.spark.compatibility\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"date\", DateType(), True),\n",
    "    StructField(\"sales_rep_id\", StringType(), True),\n",
    "    StructField(\"total_sales\", DoubleType(), True),\n",
    "    StructField(\"deals_closed\", IntegerType(), True),\n",
    "    StructField(\"customer_satisfaction_score\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Read CSV file with defined schema\n",
    "df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\") \\\n",
    "    .schema(schema) \\\n",
    "    .load(\"copy-s3-path-from-previous-section\")\n",
    "\n",
    "# Show the first few rows\n",
    "df.show(5)\n",
    "\n",
    "# Print the schema to verify\n",
    "df.printSchema()\n",
    "\n",
    "# Get basic statistics of the numerical columns\n",
    "df.describe().show()\n",
    "\n",
    "# Count total rows\n",
    "print(f\"Total number of records: {df.count()}\")\n",
    "\n",
    "df.createOrReplaceTempView(\"view_salesrep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T12:09:53.936749Z",
     "iopub.status.busy": "2024-12-19T12:09:53.932284Z"
    }
   },
   "source": [
    "## Create Iceberg table in RMS catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates a new table named 'salesrep_performance' if it doesn't already exist. The table uses the Apache Iceberg format and is designed to store sales representative performance metrics. It includes columns for date, sales representative ID, total sales amount, number of closed deals, and customer satisfaction scores. The table is configured with RMS as the AWS write format through table properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T19:10:52.921225Z",
     "iopub.status.busy": "2025-04-01T19:10:52.920642Z",
     "iopub.status.idle": "2025-04-01T19:11:08.462267Z",
     "shell.execute_reply": "2025-04-01T19:11:08.461312Z",
     "shell.execute_reply.started": "2025-04-01T19:10:52.920969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame[]\n",
      "Connection: project.spark.compatibility | Run start time: 2025-04-01 19:10:53.648018 | Run duration : 0:00:14.810925s.\n"
     ]
    }
   ],
   "source": [
    "%%pyspark project.spark.compatibility\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS salesrep_performance (\n",
    "    date DATE,\n",
    "    sales_rep_id  STRING,\n",
    "    total_sales FLOAT,\n",
    "    deals_closed INTEGER,\n",
    "    customer_satisfaction_score FLOAT\n",
    ")USING iceberg TBLPROPERTIES ('aws.write.format'='RMS')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add data into iceberg table from temporary view"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This SQL command inserts all records from the view named \"view_salesrep\" into a table called \"salesrep_performance\" using Spark SQL syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T19:11:22.567927Z",
     "iopub.status.busy": "2025-04-01T19:11:22.567478Z",
     "iopub.status.idle": "2025-04-01T19:11:40.146272Z",
     "shell.execute_reply": "2025-04-01T19:11:40.145376Z",
     "shell.execute_reply.started": "2025-04-01T19:11:22.567901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame[]\n",
      "Connection: project.spark.compatibility | Run start time: 2025-04-01 19:11:23.054979 | Run duration : 0:00:17.088057s.\n"
     ]
    }
   ],
   "source": [
    "%%pyspark project.spark.compatibility\n",
    "spark.sql(\"insert into salesrep_performance select * from view_salesrep\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
